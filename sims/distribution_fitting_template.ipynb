{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e23eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import scipy.stats \n",
    "from sklearn.neighbors import KernelDensity\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.optimize import curve_fit\n",
    "import copy\n",
    "import dill\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "chr_lengths= {1: 643000,\n",
    "                         2: 947000,\n",
    "                         3: 1100000,\n",
    "                         4: 1200000,\n",
    "                         5: 1350000,\n",
    "                         6: 1420000,\n",
    "                         7: 1450000,\n",
    "                         8: 1500000,\n",
    "                         9: 1550000,\n",
    "                         10: 1700000,\n",
    "                         11: 2049999,\n",
    "                         12: 2300000,\n",
    "                         13: 2950000,\n",
    "                         14: 3300000}\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32,\n",
    "                              np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):  #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "cpalette1 = sns.color_palette('Reds_r', 2)\n",
    "cpalette2 = sns.color_palette('Blues_r', 3)\n",
    "cpalette3 = sns.color_palette('Greys_r', 3)\n",
    "\n",
    "color_map_dict = {'PC': cpalette1[0],\n",
    "                 'FS': cpalette1[1],\n",
    "                 'MS': cpalette1[1],\n",
    "                 'GC': cpalette2[0],\n",
    "                 'HS': cpalette2[1],\n",
    "                  'FAV': cpalette2[2],\n",
    "                  'GGC': cpalette3[0],\n",
    "                  'HAV': cpalette3[1],\n",
    "                  'FCS': cpalette3[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcebab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0_11.3/fs9_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs4_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs6_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs8_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs5_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs7_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs1_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs3_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs10_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs2_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/fs4_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs3_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs10_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs5_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs2_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs1_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs9_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs6_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs7_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs8_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/fs7_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs3_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs9_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs4_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs10_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs5_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs8_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs2_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs1_2.0_11.3_r_totals.json\n",
      "2.0_11.3/fs6_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms2_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms3_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms1_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms10_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms7_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms5_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms8_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms6_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms4_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms9_2.0_11.3_ibd_segment_numbers.json\n",
      "2.0_11.3/ms10_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms5_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms2_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms4_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms3_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms7_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms8_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms1_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms9_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms6_2.0_11.3_ibd_segment_max.json\n",
      "2.0_11.3/ms3_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms9_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms4_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms7_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms1_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms6_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms5_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms8_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms10_2.0_11.3_r_totals.json\n",
      "2.0_11.3/ms2_2.0_11.3_r_totals.json\n"
     ]
    }
   ],
   "source": [
    "def combine_results(pattern, metric):\n",
    "    if metric != 'r_totals':\n",
    "        combined_dict = defaultdict(lambda: defaultdict(list))\n",
    "    else:\n",
    "        combined_dict = defaultdict(list)\n",
    "    for file in glob.glob(pattern):\n",
    "        file_dict = json.load(open(file))\n",
    "        print(file)\n",
    "        for relationship in file_dict:\n",
    "            if metric != 'r_totals':\n",
    "                for chrom in file_dict[relationship]:\n",
    "                    for segment_lengths in file_dict[relationship][chrom]:\n",
    "                        combined_dict[relationship][chrom].append(segment_lengths)\n",
    "            else:\n",
    "                combined_dict[relationship] += file_dict[relationship]\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "def combine_sims(directory):\n",
    "    fs_file_patterns = ['fs*_ibd_segment_numbers.json', 'fs*_ibd_segment_max.json',\n",
    "               'fs*_r_totals.json']\n",
    "    combined_dicts = {}\n",
    "    for pattern in fs_file_patterns:\n",
    "        match = re.search('fs\\*_(.+).json',pattern)\n",
    "        metric = match.groups()[0]\n",
    "        combined_dicts[metric] = combine_results(directory + pattern, metric)\n",
    "        json.dump(combined_dicts[metric], open(directory + 'fs_{m}_combined.json'.format(m=metric), 'w'), cls = NumpyEncoder)\n",
    "    \n",
    "    ms_file_patterns = ['ms*_ibd_segment_numbers.json', 'ms*_ibd_segment_max.json',\n",
    "               'ms*_r_totals.json']\n",
    "\n",
    "    ms_combined_dicts = {}\n",
    "    for pattern in ms_file_patterns:\n",
    "        match = re.search('ms\\*_(.+).json',pattern)\n",
    "        metric = match.groups()[0]\n",
    "        ms_combined_dicts[metric] = combine_results(directory + pattern, metric)\n",
    "        json.dump(ms_combined_dicts[metric], open(directory + 'ms_{m}_combined.json'.format(m=metric), 'w'), cls = NumpyEncoder)\n",
    "\n",
    "combine_sims('2.0_11.3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "\n",
    "for param_combo in ['2.0_11.3']:\n",
    "    sim_types = ['fs', 'ms']\n",
    "    raw_sim_data = defaultdict(dict)\n",
    "    for sim_type in sim_types:    \n",
    "        if sim_type == 'fs':\n",
    "            raw_sim_data[sim_type]['r_totals'] = json.load(open('{p}/fs_r_totals_combined.json'.format(p=param_combo)))\n",
    "            raw_sim_data[sim_type]['ibd_segment_max'] = json.load(open('{p}/fs_ibd_segment_max_combined.json'.format(p=param_combo)))\n",
    "            raw_sim_data[sim_type]['ibd_segment_numbers'] = json.load(open('{p}/fs_ibd_segment_numbers_combined.json'.format(p=param_combo)))\n",
    "        else:\n",
    "            raw_sim_data[sim_type]['r_totals'] = json.load(open('{p}/ms_r_totals_combined.json'.format(p=param_combo)))\n",
    "            raw_sim_data[sim_type]['ibd_segment_max'] = json.load(open('{p}/ms_ibd_segment_max_combined.json'.format(p=param_combo)))\n",
    "            raw_sim_data[sim_type]['ibd_segment_numbers'] = json.load(open('{p}/ms_ibd_segment_numbers_combined.json'.format(p=param_combo)))\n",
    "        max_segment_p_dict = defaultdict(dict)\n",
    "        for relationship in raw_sim_data[sim_type]['ibd_segment_max']:\n",
    "            for chrom in raw_sim_data[sim_type]['ibd_segment_max'][relationship]:\n",
    "                max_segment_p_dict[relationship][chrom] = np.asarray([x for x in np.asarray(raw_sim_data[sim_type]['ibd_segment_max'][relationship][chrom]) / chr_lengths[int(chrom)]])\n",
    "        raw_sim_data[sim_type]['p_ibd_max'] = max_segment_p_dict\n",
    "    param_dict[param_combo] = copy.deepcopy(raw_sim_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d999e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_max_segment_piecewise_pdf(x, relationship, chrom, pdf_dict, bandwidth =0.02):\n",
    "    '''modeled as a kde with spikes'''\n",
    "    #print(fs_max_segment_pdf_dict[relationship][int(chrom)])\n",
    "    p0,p1,kde, tstep, = pdf_dict[relationship][int(chrom)]\n",
    "                       \n",
    "    if float(x) <= 0 + bandwidth:\n",
    "        return p0/tstep\n",
    "    elif float(x) >= 1 - bandwidth:\n",
    "        return p1/tstep\n",
    "    else:\n",
    "        x_transmute = np.asarray([x])\n",
    "        #if beta_dis:\n",
    "        #    pdf = scipy.stats.beta.pdf(x, alpha, beta, loc, scale)\n",
    "        #else:\n",
    "        pdf = np.exp(kde.score_samples(x_transmute.reshape(-1,1)))[0]\n",
    "        pdf = pdf * (1-p0-p1)\n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9549cbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_beta(r_dict):\n",
    "    beta_variables = {}\n",
    "    for relationship in r_dict:\n",
    "        r_list = r_dict[relationship]\n",
    "        r_list = [x if x != 0 else 1e-9 for x in r_list]\n",
    "        alpha, beta, loc, scale = scipy.stats.beta.fit(r_list, floc=0,fscale=1)\n",
    "        #x = np.linspace(0, 1, 100)\n",
    "        beta_variables[relationship] = (alpha, beta, loc, scale)\n",
    "        \n",
    "    return beta_variables\n",
    "\n",
    "def create_pdfs(p_max_segment_dict, bandwidth= 0.02):\n",
    "    max_segment_pdf_dict = defaultdict(dict)\n",
    "    for relationship in p_max_segment_dict:\n",
    "        for chromosome in p_max_segment_dict[relationship]:\n",
    "            data = np.asarray(p_max_segment_dict[relationship][chromosome])\n",
    "            mask0 = np.asarray(p_max_segment_dict[relationship][chromosome]) <= 0 + bandwidth\n",
    "            mask1 = np.asarray(p_max_segment_dict[relationship][chromosome]) >= 1 - bandwidth\n",
    "            mask = ~mask0 & ~mask1\n",
    "            data = np.asarray(data[mask])\n",
    "\n",
    "            p0 = (np.sum(mask0) + 1 )/ (len(mask0) + 3)\n",
    "            p1 = (np.sum(mask1) + 1 )/ (len(mask1) + 3)\n",
    "            kde = KernelDensity(kernel='gaussian',bandwidth=bandwidth).fit(data.reshape(-1,1))#training of model\n",
    "            #beta_params = scipy.stats.beta.fit(data)\n",
    "\n",
    "            max_segment_pdf_dict[relationship][int(chromosome)] =  (p0, p1, kde, bandwidth)#, beta_params)\n",
    "    return max_segment_pdf_dict\n",
    "\n",
    "def calc_seg_count_pmf(segment_counts_dict):\n",
    "    '''empirical segment_count_pmf, with add one smoothing\n",
    "    theta_i = (x_i + alpha) / (N + alpha * d)\n",
    "    where x_i is the count of the cateogory i\n",
    "    alpha is set to 1\n",
    "    d is the number of categories\n",
    "    N is the sample count'''\n",
    "    seg_count_pmf = defaultdict(lambda: defaultdict(lambda : defaultdict(dict)))\n",
    "    for relationship in segment_counts_dict:\n",
    "        for chrom in segment_counts_dict[relationship]:\n",
    "            counts = Counter(segment_counts_dict[relationship][chrom])\n",
    "            total = np.sum(list(counts.values()))\n",
    "            max_count= max(counts)\n",
    "            bins = range(0,max_count + 2) #add an additional category representing max + 1\n",
    "            n_bins = len(bins)\n",
    "            for x in bins:\n",
    "                if x in counts:\n",
    "                    seg_count_pmf[relationship][chrom][x] = (counts[x] + 1)/(total + n_bins)\n",
    "                else:\n",
    "                    seg_count_pmf[relationship][chrom]['misc'] = 1 / (total + n_bins)\n",
    "    return seg_count_pmf\n",
    "\n",
    "def evaluate_max_segment_piecewise_pdf(x, relationship, chrom, pdf_dict, bandwidth =0.02):\n",
    "    '''modeled as a kde with spikes'''\n",
    "    #print(fs_max_segment_pdf_dict[relationship][int(chrom)])\n",
    "    p0,p1,kde, tstep, = pdf_dict[relationship][int(chrom)]\n",
    "                       \n",
    "    if float(x) <= 0 + bandwidth:\n",
    "        return p0/tstep\n",
    "    elif float(x) >= 1 - bandwidth:\n",
    "        return p1/tstep\n",
    "    else:\n",
    "        x_transmute = np.asarray([x])\n",
    "        #if beta_dis:\n",
    "        #    pdf = scipy.stats.beta.pdf(x, alpha, beta, loc, scale)\n",
    "        #else:\n",
    "        pdf = np.exp(kde.score_samples(x_transmute.reshape(-1,1)))[0]\n",
    "        pdf = pdf * (1-p0-p1)\n",
    "        return pdf\n",
    "\n",
    "\n",
    "    \n",
    "def create_joint_kde(r_dict,l_dict, n_dict):\n",
    "    kde_dict = {}\n",
    "    for G in r_dict:\n",
    "        consolidated_data = DataFrame()\n",
    "        consolidated_data['r_total'] = r_dict[G]\n",
    "        for chrom in range(1,15):\n",
    "            consolidated_data['max_ibd_{x}'.format(x=chrom)] = l_dict[G][str(chrom)]\n",
    "            consolidated_data['n_segment_{x}'.format(x=chrom)] = n_dict[G][str(chrom)]\n",
    "\n",
    "        data = np.asarray(consolidated_data)[:4000]\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth = 0.01).fit(data)\n",
    "        kde_dict[G] = kde\n",
    "    return kde_dict\n",
    "\n",
    "\n",
    "    \n",
    "raw_sim_data = param_dict['2.0_11.3']\n",
    "tmp_pdfs = defaultdict(dict)\n",
    "for sim_type in raw_sim_data:\n",
    "    tmp_pdfs[sim_type]['r_beta'] = fit_beta(raw_sim_data[sim_type]['r_totals'])\n",
    "    tmp_pdfs[sim_type]['p_max_segment'] = create_pdfs(raw_sim_data[sim_type]['p_ibd_max'])\n",
    "    tmp_pdfs[sim_type]['segment_count'] = calc_seg_count_pmf(raw_sim_data[sim_type]['ibd_segment_numbers'])\n",
    "    tmp_pdfs[sim_type]['joint'] = create_joint_kde(raw_sim_data[sim_type]['r_totals'],\n",
    "                                                 raw_sim_data[sim_type]['p_ibd_max'],\n",
    "                                                 raw_sim_data[sim_type]['ibd_segment_numbers'])\n",
    "    \n",
    "pdfs = {}\n",
    "pdfs['r_beta'] = tmp_pdfs['fs']['r_beta']\n",
    "pdfs['p_max_segment'] = tmp_pdfs['fs']['p_max_segment']\n",
    "pdfs['segment_count'] = tmp_pdfs['fs']['segment_count']\n",
    "\n",
    "for key in pdfs:\n",
    "    pdfs[key].pop('MS')\n",
    "\n",
    "for G in ['MS', 'FAV', 'FCS']: #subset of relationships that involve the MS and are thus affected\n",
    "    pdfs['r_beta'][G + '.MS'] = tmp_pdfs['ms']['r_beta'][G]\n",
    "    pdfs['p_max_segment'][G + '.MS'] = tmp_pdfs['ms']['p_max_segment'][G]\n",
    "    pdfs['segment_count'][G + '.MS'] = tmp_pdfs['ms']['segment_count'][G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43247c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sim_parameters.pkl', 'wb') as fp:\n",
    "    dill.dump(pdfs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "84c1d519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "GC\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "GGC\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "FS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "HS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "FAV\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "HAV\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "FCS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "MS.MS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "FAV.MS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "FCS.MS\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "class Sim:\n",
    "    def __init__(self, relationship, r_total, max_ibd_segment, n_segment_count):\n",
    "        self.relationship = relationship\n",
    "        self.r_total = r_total\n",
    "\n",
    "        self.max_ibd_segment = max_ibd_segment\n",
    "        self.n_segment_count = n_segment_count\n",
    "        self.calc_all_likelihoods()\n",
    "    \n",
    "    def likelihood(self, G, r_flag = 1, ibdmax_flag = 1, count_flag = 1):\n",
    "        r_total_beta_params = pdfs['r_beta'][G]\n",
    "        logL = 0\n",
    "        P_rtotal = scipy.stats.beta.logpdf(self.r_total, *r_total_beta_params)\n",
    "        P_ibdmax = 0\n",
    "        P_seg_count = 0\n",
    "        for chrom in range(1,15):\n",
    "            idx = chrom - 1\n",
    "            P_ibdmax += np.log(evaluate_max_segment_piecewise_pdf(self.max_ibd_segment[idx], G, chrom, \\\n",
    "                                                            pdfs['p_max_segment']))\n",
    "\n",
    "            n_segments = self.n_segment_count[idx]\n",
    "            if n_segments in pdfs['segment_count'][G][str(chrom)].keys():\n",
    "                P_seg_count += np.log(pdfs['segment_count'][G][str(chrom)][n_segments])\n",
    "            else:\n",
    "                P_seg_count += np.log(pdfs['segment_count'][G][str(chrom)]['misc'])\n",
    "        logL = P_rtotal * r_flag +  P_ibdmax * ibdmax_flag + P_seg_count * count_flag\n",
    "\n",
    "        return logL\n",
    "    \n",
    "    def calc_all_likelihoods(self, r_flag = 1, ibdmax_flag = 1, count_flag = 1):\n",
    "        self.complete_likelihoods = {}\n",
    "        self.r_likelihoods = {}\n",
    "        for G in ['PC', 'GC', 'GGC', 'FS', 'HS', 'FAV', 'HAV', 'FCS', 'MS.MS', 'FAV.MS', 'FCS.MS']:\n",
    "            self.complete_likelihoods[G] = self.likelihood(G, r_flag, ibdmax_flag, count_flag)\n",
    "            self.r_likelihoods[G] = self.likelihood(G, 1, 0, 0)\n",
    "        \n",
    "        self.max_complete_likelihood = max(self.complete_likelihoods, key=self.complete_likelihoods.get)\n",
    "        self.max_r_likelihood = max(self.r_likelihoods, key=self.r_likelihoods.get)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "n_sims = len(raw_sim_data['fs']['ibd_segment_numbers']['PC']['1'])\n",
    "sim_indiv = defaultdict(list)\n",
    "sim_type = 'fs'\n",
    "for relationship in ['PC', 'GC', 'GGC', 'FS', 'HS', 'FAV', 'HAV', 'FCS']:\n",
    "    print(relationship)\n",
    "    for sim_idx in range(n_sims):\n",
    "        if sim_idx % 500 == 0:\n",
    "            print(sim_idx)\n",
    "        max_segment_list = [raw_sim_data[sim_type]['p_ibd_max'][relationship][str(chrom)][sim_idx] for chrom in range(1,15)]\n",
    "        count_list = [raw_sim_data[sim_type]['ibd_segment_numbers'][relationship][str(chrom)][sim_idx] for chrom in range(1,15)]\n",
    "        \n",
    "        sim_indiv[relationship].append(Sim(relationship, raw_sim_data[sim_type]['r_totals'][relationship][sim_idx], \\\n",
    "                                      max_segment_list, count_list))\n",
    "        \n",
    "sim_type = 'ms'\n",
    "for relationship in ['MS.MS', 'FAV.MS', 'FCS.MS']:\n",
    "    print(relationship)\n",
    "    for sim_idx in range(n_sims):\n",
    "        if sim_idx % 500 == 0:\n",
    "            print(sim_idx)\n",
    "        base_r = relationship.split('.')[0]\n",
    "        max_segment_list = [raw_sim_data[sim_type]['p_ibd_max'][base_r][str(chrom)][sim_idx] for chrom in range(1,15)]\n",
    "        count_list = [raw_sim_data[sim_type]['ibd_segment_numbers'][base_r][str(chrom)][sim_idx] for chrom in range(1,15)]\n",
    "        \n",
    "        sim_indiv[relationship].append(Sim(relationship, raw_sim_data[sim_type]['r_totals'][base_r][sim_idx], \\\n",
    "                                      max_segment_list, count_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
